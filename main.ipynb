{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "focused-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project: p7\n",
    "# submitter: lliu356\n",
    "# partner: none\n",
    "# hours: 4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "import netaddr\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class UserPredictor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Pipeline([('both',make_column_transformer((OneHotEncoder(),['badge']),\n",
    "                                                               (OneHotEncoder(handle_unknown = 'ignore'),['region']),\n",
    "                                                               (PolynomialFeatures(degree = 2,include_bias=False),['weighted_time']),\n",
    "                                                               remainder = \"passthrough\")),\n",
    "                               ('cls', LogisticRegression(fit_intercept=False))])       \n",
    "        self.features_cols = ['region','badge','age','past_purchase_amt','weighted_time']\n",
    "        self.y = 'y'\n",
    "        \n",
    "    def process_raw(self,users,logs, y = None):\n",
    "        ### user:\n",
    "        user_tmp = users #.copy()\n",
    "        \n",
    "        ### logs: match region\n",
    "        ip2check_test = list(logs['ip_address'])\n",
    "        ip_df = self.ip2location_load()\n",
    "        match_region_out = self.ip_check(ip2check_test,ip_df)\n",
    "        logs_add_region = logs #.copy()\n",
    "        logs_add_region['region'] = match_region_out\n",
    "        logs_match_region = logs_add_region[['id','region']].drop_duplicates()\n",
    "        \n",
    "        ### logs: match weigted time \n",
    "        page_type = list(logs['url_visited'])\n",
    "        \n",
    "        ### trnsform url (manually)\n",
    "        url_transform = []\n",
    "        #url_ref = {}\n",
    "        for i in range(0,len(page_type)):\n",
    "            tmp = page_type[i].replace('.html','').replace('/','')\n",
    "            url_transform.append(tmp)\n",
    "            #url_ref[tmp] = url_ref.setdefault(tmp,0) + 1\n",
    "        \n",
    "        # Any better idea than hardcoding these categories...?\n",
    "        laptop = ['laptop']; office = ['tablet','keyboard','monitor','printer','desk']\n",
    "        \n",
    "        ### weight time\n",
    "        page_type = list(logs['url_visited'])\n",
    "        minutes_raw = list(logs['minutes_on_page'])\n",
    "        minutes_weighted = [None] * len(minutes_raw)\n",
    "        \n",
    "        for i in range(0,len(page_type)):\n",
    "            tmp = page_type[i].replace('.html','').replace('/','')\n",
    "            if tmp in laptop:\n",
    "                minutes_weighted[i] = minutes_raw[i] * 0.7\n",
    "            elif tmp in office:\n",
    "                minutes_weighted[i] = minutes_raw[i] * 0.2\n",
    "            elif tmp == 'NotFound':\n",
    "                minutes_weighted[i] = 0\n",
    "        # extract        \n",
    "        logs_match_weight = logs#.copy()\n",
    "        logs_match_weight['weighted_time'] = minutes_weighted\n",
    "        logs_match_weight = pd.DataFrame(logs_match_weight.groupby('id')['weighted_time'].sum())\n",
    "        \n",
    "        ### merge:\n",
    "        user_wtime = pd.merge(user_tmp,logs_match_weight,on = 'id',how='left').fillna(0)\n",
    "        out = pd.merge(user_wtime,logs_match_region,on = 'id',how='left').fillna('NotFound')\n",
    "        if not y is None:\n",
    "            out = pd.merge(out,y,how='left',on = 'id')\n",
    "            out['y'] = out['y'].map({0:False,1:True})\n",
    "            return out\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def fit(self, train_users, train_logs, train_y, crs_val = False):\n",
    "        \n",
    "        self.features_train = self.process_raw(train_users, train_logs, train_y)\n",
    "        \n",
    "        self.model.fit(self.features_train[self.features_cols],self.features_train[self.y])\n",
    "        \n",
    "        if crs_val:\n",
    "            crs_val_scores  = cross_val_score(self.model,\n",
    "                                              self.features_train[self.features_cols],\n",
    "                                              self.features_train[self.y])\n",
    "            print(f\"AVG: {crs_val_scores.mean()}, STD: {crs_val_scores.std()}\\n\")\n",
    "            \n",
    "    def predict(self,test_users, test_logs):\n",
    "        self.features_test = self.process_raw(test_users, test_logs)\n",
    "        y_predicted = self.model.predict(self.features_test[self.features_cols])\n",
    "                       \n",
    "        return np.asarray(list(map(int, y_predicted)))\n",
    "    \n",
    "    def BinarySearch(self,arr,x):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left < right:\n",
    "            mid = left + (right - left) // 2\n",
    "            if x - arr[mid] > arr[mid + 1] - x:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid\n",
    "        if arr[left-1] <= x and arr[left] > x:\n",
    "            return left - 1\n",
    "        else:\n",
    "            return left\n",
    "                       \n",
    "    def ip2location_load(self):\n",
    "        with open(os.path.join('data','ip2location.csv')) as f:\n",
    "            out_raw = f.read()\n",
    "        out = out_raw.split(\"\\n\")\n",
    "        ip_raw = []\n",
    "        for line in out:\n",
    "            line_tmp = line.split(\",\")\n",
    "            ip_raw.append(line_tmp)    \n",
    "        ip_df = pd.DataFrame(ip_raw)\n",
    "        ip_df.rename(columns = ip_df.iloc[0], inplace = True)\n",
    "        ip_df.drop(ip_df.index[0],inplace = True)\n",
    "        ip_df.drop(ip_df.index[-1],inplace = True) # something weird\n",
    "        ip_df.sort_values(by = [\"low\"]) # sort for binary search\n",
    "        ip_df.reset_index(drop = True,inplace = True)\n",
    "        return ip_df\n",
    "    \n",
    "    def ip_check(self,ips,ip_df):\n",
    "        search_sequence = list(map(int, ip_df[\"low\"]))\n",
    "        ip_match_out = []\n",
    "        for ip in ips:    \n",
    "            try:\n",
    "                int_ip = int(netaddr.IPAddress(ip))\n",
    "                index_tmp = self.BinarySearch(search_sequence,int_ip)\n",
    "                matched_region = ip_df.iloc[index_tmp,3]\n",
    "                ip_match_out.append(matched_region)  \n",
    "            except:\n",
    "                print(\"Execution halted...\")\n",
    "                break\n",
    "        return ip_match_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nervous-caribbean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>url_visited</th>\n",
       "      <th>minutes_on_page</th>\n",
       "      <th>weighted_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6-14-2020</td>\n",
       "      <td>19083</td>\n",
       "      <td>123.196.90.97</td>\n",
       "      <td>/blender.html</td>\n",
       "      <td>7.222454</td>\n",
       "      <td>0.722245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8-22-2020</td>\n",
       "      <td>764</td>\n",
       "      <td>123.50.162.196</td>\n",
       "      <td>/cleats.html</td>\n",
       "      <td>5.472573</td>\n",
       "      <td>0.547257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9-14-2020</td>\n",
       "      <td>733</td>\n",
       "      <td>192.141.247.60</td>\n",
       "      <td>/tablet.html</td>\n",
       "      <td>8.396012</td>\n",
       "      <td>1.679202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8-27-2020</td>\n",
       "      <td>16282</td>\n",
       "      <td>203.28.112.62</td>\n",
       "      <td>/keyboard.html</td>\n",
       "      <td>4.139941</td>\n",
       "      <td>0.827988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-29-2020</td>\n",
       "      <td>5694</td>\n",
       "      <td>23.78.141.31</td>\n",
       "      <td>/cleats.html</td>\n",
       "      <td>11.204377</td>\n",
       "      <td>1.120438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>7-14-2020</td>\n",
       "      <td>10299</td>\n",
       "      <td>108.61.207.238</td>\n",
       "      <td>/cooler.html</td>\n",
       "      <td>17.208746</td>\n",
       "      <td>1.720875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>4-17-2020</td>\n",
       "      <td>981</td>\n",
       "      <td>45.124.21.4</td>\n",
       "      <td>/basketball.html</td>\n",
       "      <td>10.926431</td>\n",
       "      <td>1.092643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>9-27-2020</td>\n",
       "      <td>10451</td>\n",
       "      <td>103.15.97.131</td>\n",
       "      <td>/monitor.html</td>\n",
       "      <td>11.502494</td>\n",
       "      <td>2.300499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>4-13-2020</td>\n",
       "      <td>2885</td>\n",
       "      <td>203.26.56.100</td>\n",
       "      <td>/keyboard.html</td>\n",
       "      <td>5.056544</td>\n",
       "      <td>1.011309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>10-25-2020</td>\n",
       "      <td>16255</td>\n",
       "      <td>49.128.5.56</td>\n",
       "      <td>/backpack.html</td>\n",
       "      <td>2.326346</td>\n",
       "      <td>0.232635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     id      ip_address       url_visited  minutes_on_page  \\\n",
       "0        6-14-2020  19083   123.196.90.97     /blender.html         7.222454   \n",
       "1        8-22-2020    764  123.50.162.196      /cleats.html         5.472573   \n",
       "2        9-14-2020    733  192.141.247.60      /tablet.html         8.396012   \n",
       "3        8-27-2020  16282   203.28.112.62    /keyboard.html         4.139941   \n",
       "4        7-29-2020   5694    23.78.141.31      /cleats.html        11.204377   \n",
       "...            ...    ...             ...               ...              ...   \n",
       "199995   7-14-2020  10299  108.61.207.238      /cooler.html        17.208746   \n",
       "199996   4-17-2020    981     45.124.21.4  /basketball.html        10.926431   \n",
       "199997   9-27-2020  10451   103.15.97.131     /monitor.html        11.502494   \n",
       "199998   4-13-2020   2885   203.26.56.100    /keyboard.html         5.056544   \n",
       "199999  10-25-2020  16255     49.128.5.56    /backpack.html         2.326346   \n",
       "\n",
       "        weighted_time  \n",
       "0            0.722245  \n",
       "1            0.547257  \n",
       "2            1.679202  \n",
       "3            0.827988  \n",
       "4            1.120438  \n",
       "...               ...  \n",
       "199995       1.720875  \n",
       "199996       1.092643  \n",
       "199997       2.300499  \n",
       "199998       1.011309  \n",
       "199999       0.232635  \n",
       "\n",
       "[200000 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(\"data/train_users.csv\")\n",
    "logs = pd.read_csv(\"data/train_logs.csv\")\n",
    "\n",
    "page_type = list(logs['url_visited'])\n",
    "\n",
    "        \n",
    "### trnsform url (manually)\n",
    "url_transform = []\n",
    "#url_ref = {}\n",
    "for i in range(0,len(page_type)):\n",
    "    tmp = page_type[i].replace('.html','').replace('/','')\n",
    "    url_transform.append(tmp)\n",
    "        \n",
    "        # Any better idea than hardcoding these categories...?\n",
    "laptop = ['laptop']; office = ['tablet','keyboard','monitor','printer','desk']\n",
    "        \n",
    "        ### weight time\n",
    "page_type = list(logs['url_visited'])\n",
    "minutes_raw = list(logs['minutes_on_page'])\n",
    "minutes_weighted = [None] * len(minutes_raw)\n",
    "        \n",
    "for i in range(0,len(page_type)):\n",
    "    tmp = page_type[i].replace('.html','').replace('/','')\n",
    "    if tmp in laptop:\n",
    "        minutes_weighted[i] = minutes_raw[i] * 0.7\n",
    "    elif tmp in office:\n",
    "        minutes_weighted[i] = minutes_raw[i] * 0.2\n",
    "    else:\n",
    "        minutes_weighted[i] = minutes_raw[i] * 0.1\n",
    "\n",
    "logs_match_weight = logs#.copy()\n",
    "logs_match_weight['weighted_time'] = minutes_weighted\n",
    "logs_match_weight = pd.DataFrame(logs_match_weight.groupby('id')['weighted_time'].sum())\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "round-accordance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.merge(users,logs_match_weight,how = 'left',on = 'id').fillna(0)\n",
    "t['weighted_time'].isnull().values.any()\n",
    "#out = pd.merge(,logs_match_weight,on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "latest-child",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id             name  age   badge  past_purchase_amt  weighted_time region\n",
      "0   1      Nikki Young   36  bronze              42.94      12.976759  Japan\n",
      "1   2    William Moats   40    gold              10.03       6.472002  China\n",
      "2   3       John Lemke   24  silver             203.37      15.156423  China\n",
      "3   4  Elizabeth Gavin   39  bronze             132.04       3.521110  Japan\n",
      "4   5     Myrtle Blais   37  bronze              41.56      12.304702  Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: 0.9243500000000001, STD: 0.00753093619678191\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liheliu/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "model = UserPredictor()\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "train_logs = pd.read_csv(\"data/train_logs.csv\")\n",
    "train_y = pd.read_csv(\"data/train_y.csv\")\n",
    "model.fit(train_users, train_logs, train_y, crs_val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "electric-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id              name  age   badge  past_purchase_amt  weighted_time  \\\n",
      "0  100001       William Lee   33  bronze              10.12       3.298134   \n",
      "1  100002     Ernest Glover   37  bronze              21.22       5.270474   \n",
      "2  100003    James Thompson   31  silver              18.02      15.125220   \n",
      "3  100004      Lillie Yates   33    gold              43.41      46.534725   \n",
      "4  100005  George Schaeffer   32  bronze              11.55       5.060739   \n",
      "\n",
      "                     region  \n",
      "0  United States of America  \n",
      "1                    Mexico  \n",
      "2                     Japan  \n",
      "3                     Japan  \n",
      "4  United States of America  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_users = pd.read_csv(\"data/test1_users.csv\")\n",
    "test_logs = pd.read_csv(\"data/test1_logs.csv\")\n",
    "y_pred = model.predict(test_users, test_logs)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-powder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
